{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference & Citation: \n",
    "https://zhuanlan.zhihu.com/p/37357981 \n",
    "https://zhuanlan.zhihu.com/p/38329631 \n",
    "https://github.com/topics/gradient-boosting-machine?l=python\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "heart = pd.read_csv(\"Heart.csv\")\n",
    "\n",
    "heart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c1d3c27934b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m#Test each model's accuracy and time consumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_hastie_10_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "class AdaBoost(object):\n",
    "    \n",
    "    def __init__(self, M, clf, learning_rate=1.0, method=\"discrete\", tol=None, weight_trimming=None):\n",
    "        self.M = M\n",
    "        self.clf = clf\n",
    "        self.learning_rate = learning_rate\n",
    "        self.method = method\n",
    "        self.tol = tol\n",
    "        self.weight_trimming = weight_trimming\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # tol is the threadhold for early_stopping; if we use early_stopping, we need to split the training set and validation set\n",
    "        if self.tol is not None:      \n",
    "            X, X_val, y, y_val = train_test_split(X, y, random_state=2)  \n",
    "            former_loss = 1\n",
    "            count = 0\n",
    "            tol_init = self.tol\n",
    "\n",
    "        w = np.array([1 / len(X)] * len(X))   # Initiate the original weight to be 1/n \n",
    "        self.clf_total = []\n",
    "        self.alpha_total = []\n",
    "\n",
    "        for m in range(self.M):\n",
    "            classifier = clone(self.clf)\n",
    "            if self.method == \"discrete\":\n",
    "                if m >= 1 and self.weight_trimming is not None:\n",
    "                    # implementation of weight_trimming: firstly, sort the weight, calculate the accumulation, then remove the weights that are too small\n",
    "                    sort_w = np.sort(w)[::-1]     \n",
    "                    cum_sum = np.cumsum(sort_w)   \n",
    "                    percent_w = sort_w[np.where(cum_sum >= self.weight_trimming)][0]   \n",
    "                    w_fit, X_fit, y_fit = w[w >= percent_w], X[w >= percent_w], y[w >= percent_w]\n",
    "                    y_pred = classifier.fit(X_fit, y_fit, sample_weight=w_fit).predict(X)\n",
    "\n",
    "                else:\n",
    "                    y_pred = classifier.fit(X, y, sample_weight=w).predict(X)\n",
    "                loss = np.zeros(len(X))\n",
    "                loss[y_pred != y] = 1\n",
    "                err = np.sum(w * loss)    # calculate the error rate with weight\n",
    "                alpha = 0.5 * np.log((1 - err) / err) * self.learning_rate  # machine learner's coefficience | alpha\n",
    "                w = (w * np.exp(-y * alpha * y_pred)) / np.sum(w * np.exp(-y * alpha * y_pred))  # Update the weight distribution.\n",
    "\n",
    "                self.alpha_total.append(alpha)\n",
    "                self.clf_total.append(classifier)\n",
    "\n",
    "            elif self.method == \"real\":\n",
    "                if m >= 1 and self.weight_trimming is not None:\n",
    "                    sort_w = np.sort(w)[::-1]\n",
    "                    cum_sum = np.cumsum(sort_w)\n",
    "                    percent_w = sort_w[np.where(cum_sum >= self.weight_trimming)][0]\n",
    "                    w_fit, X_fit, y_fit = w[w >= percent_w], X[w >= percent_w], y[w >= percent_w]\n",
    "                    y_pred = classifier.fit(X_fit, y_fit, sample_weight=w_fit).predict_proba(X)[:, 1]\n",
    "\n",
    "                else:\n",
    "                    y_pred = classifier.fit(X, y, sample_weight=w).predict_proba(X)[:, 1]  \n",
    "                y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "                clf = 0.5 * np.log(y_pred / (1 - y_pred)) * self.learning_rate\n",
    "                w = (w * np.exp(-y * clf)) / np.sum(w * np.exp(-y * clf))\n",
    "\n",
    "                self.clf_total.append(classifier)\n",
    "\n",
    "            '''early stopping'''\n",
    "            if m % 10 == 0 and m > 300 and self.tol is not None:\n",
    "                if self.method == \"discrete\":\n",
    "                    p = np.array([self.alpha_total[m] * self.clf_total[m].predict(X_val) for m in range(m)])\n",
    "                elif self.method == \"real\":\n",
    "                    p = []\n",
    "                    for m in range(m):\n",
    "                        ppp = self.clf_total[m].predict_proba(X_val)[:, 1]\n",
    "                        ppp = np.clip(ppp, 1e-15, 1 - 1e-15)\n",
    "                        p.append(self.learning_rate * 0.5 * np.log(ppp / (1 - ppp)))\n",
    "                    p = np.array(p)\n",
    "\n",
    "                stage_pred = np.sign(p.sum(axis=0))\n",
    "                later_loss = zero_one_loss(stage_pred, y_val)\n",
    "\n",
    "                if later_loss > (former_loss + self.tol):\n",
    "                    count += 1\n",
    "                    self.tol = self.tol / 2  \n",
    "                else:\n",
    "                    count = 0\n",
    "                    self.tol = tol_init\n",
    "                if count == 2:\n",
    "                    self.M = m - 20\n",
    "                    print(\"early stopping in round {}, best round is {}, M = {}\".format(m, m - 20, self.M))\n",
    "                    break\n",
    "                former_loss = later_loss\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.method == \"discrete\":\n",
    "            pred = np.array([self.alpha_total[m] * self.clf_total[m].predict(X) for m in range(self.M)])\n",
    "\n",
    "        elif self.method == \"real\":\n",
    "            pred = []\n",
    "            for m in range(self.M):\n",
    "                p = self.clf_total[m].predict_proba(X)[:, 1]\n",
    "                p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "                pred.append(0.5 * np.log(p / (1 - p)))\n",
    "\n",
    "        return np.sign(np.sum(pred, axis=0))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Test each model's accuracy and time consumed.   \n",
    "    X, y = datasets.make_hastie_10_2(n_samples=20000, random_state=1)   # data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_discrete = AdaBoost(M=2000, clf=DecisionTreeClassifier(max_depth=1, random_state=1), learning_rate=1.0, \n",
    "                              method=\"discrete\", weight_trimming=None)\n",
    "    model_discrete.fit(X_train, y_train)\n",
    "    pred_discrete = model_discrete.predict(X_test)\n",
    "    acc = np.zeros(pred_discrete.shape)\n",
    "    acc[np.where(pred_discrete == y_test)] = 1\n",
    "    accuracy = np.sum(acc) / len(pred_discrete)\n",
    "    print('Discrete Adaboost accuracy: ', accuracy)\n",
    "    print('Discrete Adaboost time: ', '{:.2f}'.format(time.time() - start_time),'\\n')\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_real = AdaBoost(M=2000, clf=DecisionTreeClassifier(max_depth=1, random_state=1), learning_rate=1.0, \n",
    "                          method=\"real\", weight_trimming=None)\n",
    "    model_real.fit(X_train, y_train)\n",
    "    pred_real = model_real.predict(X_test)\n",
    "    acc = np.zeros(pred_real.shape)\n",
    "    acc[np.where(pred_real == y_test)] = 1\n",
    "    accuracy = np.sum(acc) / len(pred_real)\n",
    "    print('Real Adaboost accuracy: ', accuracy)  \n",
    "    print(\"Real Adaboost time: \", '{:.2f}'.format(time.time() - start_time),'\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_discrete_weight = AdaBoost(M=2000, clf=DecisionTreeClassifier(max_depth=1, random_state=1), learning_rate=1.0, \n",
    "                                     method=\"discrete\", weight_trimming=0.995)\n",
    "    model_discrete_weight.fit(X_train, y_train)\n",
    "    pred_discrete_weight = model_discrete_weight.predict(X_test)\n",
    "    acc = np.zeros(pred_discrete_weight.shape)\n",
    "    acc[np.where(pred_discrete_weight == y_test)] = 1\n",
    "    accuracy = np.sum(acc) / len(pred_discrete_weight)\n",
    "    print('Discrete Adaboost(weight_trimming 0.995) accuracy: ', accuracy)\n",
    "    print('Discrete Adaboost(weight_trimming 0.995) time: ', '{:.2f}'.format(time.time() - start_time),'\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "    mdoel_real_weight = AdaBoost(M=2000, clf=DecisionTreeClassifier(max_depth=1, random_state=1), learning_rate=1.0, \n",
    "                                     method=\"real\", weight_trimming=0.999)\n",
    "    mdoel_real_weight.fit(X_train, y_train)\n",
    "    pred_real_weight = mdoel_real_weight.predict(X_test)\n",
    "    acc = np.zeros(pred_real_weight.shape)\n",
    "    acc[np.where(pred_real_weight == y_test)] = 1\n",
    "    accuracy = np.sum(acc) / len(pred_real_weight)\n",
    "    print('Real Adaboost(weight_trimming 0.999) accuracy: ', accuracy)\n",
    "    print('Real Adaboost(weight_trimming 0.999) time: ', '{:.2f}'.format(time.time() - start_time),'\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_discrete = AdaBoost(M=2000, clf=DecisionTreeClassifier(max_depth=1, random_state=1), learning_rate=1.0, \n",
    "                              method=\"discrete\", weight_trimming=None, tol=0.0001)\n",
    "    model_discrete.fit(X_train, y_train)\n",
    "    pred_discrete = model_discrete.predict(X_test)\n",
    "    acc = np.zeros(pred_discrete.shape)\n",
    "    acc[np.where(pred_discrete == y_test)] = 1\n",
    "    accuracy = np.sum(acc) / len(pred_discrete)\n",
    "    print('Discrete Adaboost accuracy (early_stopping): ', accuracy)\n",
    "    print('Discrete Adaboost time (early_stopping): ', '{:.2f}'.format(time.time() - start_time),'\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_real = AdaBoost(M=2000, clf=DecisionTreeClassifier(max_depth=1, random_state=1), learning_rate=1.0, \n",
    "                          method=\"real\", weight_trimming=None, tol=0.0001)\n",
    "    model_real.fit(X_train, y_train)\n",
    "    pred_real = model_real.predict(X_test)\n",
    "    acc = np.zeros(pred_real.shape)\n",
    "    acc[np.where(pred_real == y_test)] = 1\n",
    "    accuracy = np.sum(acc) / len(pred_real)\n",
    "    print('Real Adaboost accuracy (early_stopping): ', accuracy)  \n",
    "    print('Discrete Adaboost time (early_stopping): ', '{:.2f}'.format(time.time() - start_time),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, we have to define the various loss function; logistic loss，modified huber loss\n",
    "def SquaredLoss_NegGradient(y_pred, y):\n",
    "    return y - y_pred\n",
    "\n",
    "def Huberloss_NegGradient(y_pred, y, alpha):\n",
    "    diff = y - y_pred\n",
    "    delta = stats.scoreatpercentile(np.abs(diff), alpha * 100)\n",
    "    g = np.where(np.abs(diff) > delta, delta * np.sign(diff), diff)\n",
    "    return g\n",
    "\n",
    "def logistic(p):\n",
    "    return 1 / (1 + np.exp(-2 * p))\n",
    "\n",
    "def LogisticLoss_NegGradient(y_pred, y):\n",
    "    g = 2 * y / (1 + np.exp(1 + 2 * y * y_pred))  # logistic_loss = log(1+exp(-2*y*y_pred))\n",
    "    return g\n",
    "\n",
    "def modified_huber(p):\n",
    "    return (np.clip(p, -1, 1) + 1) / 2\n",
    "\n",
    "def Modified_Huber_NegGradient(y_pred, y):\n",
    "    margin = y * y_pred\n",
    "    g = np.where(margin >= 1, 0, np.where(margin >= -1, y * 2 * (1-margin), 4 * y))\n",
    "    # modified_huber_loss = np.where(margin >= -1, max(0, (1-margin)^2), -4 * margin)\n",
    "    return g\n",
    "\n",
    "\n",
    "class GradientBoosting(object):\n",
    "    def __init__(self, M, base_learner, learning_rate=1.0, method=\"regression\", tol=None, subsample=None,\n",
    "                 loss=\"square\", alpha=0.9):\n",
    "        self.M = M\n",
    "        self.base_learner = base_learner\n",
    "        self.learning_rate = learning_rate\n",
    "        self.method = method\n",
    "        self.tol = tol\n",
    "        self.subsample = subsample\n",
    "        self.loss = loss\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "  \n",
    "        if self.tol is not None:\n",
    "            X, X_val, y, y_val = train_test_split(X, y, random_state=2)\n",
    "            former_loss = float(\"inf\")\n",
    "            count = 0\n",
    "            tol_init = self.tol\n",
    "\n",
    "        init_learner = self.base_learner\n",
    "        y_pred = init_learner.fit(X, y).predict(X)   # initial value\n",
    "        self.base_learner_total = [init_learner]\n",
    "        for m in range(self.M):\n",
    "\n",
    "            if self.subsample is not None:  # subsample\n",
    "                sample = [np.random.choice(len(X), int(self.subsample * len(X)), replace=False)]\n",
    "                X_s, y_s, y_pred_s = X[sample], y[sample], y_pred[sample]  \n",
    "            else:\n",
    "                X_s, y_s, y_pred_s = X, y, y_pred\n",
    "\n",
    "            # calculate the negative gradient\n",
    "            if self.method == \"regression\":\n",
    "                if self.loss == \"square\":\n",
    "                    response = SquaredLoss_NegGradient(y_pred_s, y_s)\n",
    "                elif self.loss == \"huber\":\n",
    "                    response = Huberloss_NegGradient(y_pred_s, y_s, self.alpha)\n",
    "            elif self.method == \"classification\":\n",
    "                if self.loss == \"logistic\":\n",
    "                    response = LogisticLoss_NegGradient(y_pred_s, y_s)\n",
    "                elif self.loss == \"modified_huber\":\n",
    "                    response = Modified_Huber_NegGradient(y_pred_s, y_s)\n",
    "\n",
    "            base_learner = clone(self.base_learner)\n",
    "            y_pred += base_learner.fit(X_s, response).predict(X) * self.learning_rate\n",
    "            self.base_learner_total.append(base_learner)\n",
    "\n",
    "            '''early stopping'''\n",
    "            if m % 10 == 0 and m > 300 and self.tol is not None:\n",
    "                p = np.array([self.base_learner_total[m].predict(X_val) for m in range(1, m+1)])\n",
    "                p = np.vstack((self.base_learner_total[0].predict(X_val), p))\n",
    "                stage_pred = np.sum(p, axis=0)\n",
    "                if self.method == \"regression\":\n",
    "                    later_loss = np.sqrt(mean_squared_error(stage_pred, y_val))\n",
    "                if self.method == \"classification\":\n",
    "                    stage_pred = np.where(logistic(stage_pred) >= 0.5, 1, -1)  \n",
    "                    later_loss = zero_one_loss(stage_pred, y_val)\n",
    "\n",
    "                if later_loss > (former_loss + self.tol):\n",
    "                    count += 1\n",
    "                    self.tol = self.tol / 2  \n",
    "                    print(self.tol)          \n",
    "                else:\n",
    "                    count = 0\n",
    "                    self.tol = tol_init\n",
    "\n",
    "                if count == 2:\n",
    "                    self.M = m - 20\n",
    "                    print(\"early stopping in round {}, best round is {}, M = {}\".format(m, m - 20, self.M))\n",
    "                    break\n",
    "                former_loss = later_loss\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = np.array([self.base_learner_total[m].predict(X) * self.learning_rate for m in range(1, self.M + 1)])\n",
    "        pred = np.vstack((self.base_learner_total[0].predict(X), pred))  \n",
    "        if self.method == \"regression\":\n",
    "            pred_final = np.sum(pred, axis=0)\n",
    "        elif self.method == \"classification\":\n",
    "            if self.loss == \"modified_huber\":\n",
    "                p = np.sum(pred, axis=0)\n",
    "                pred_final = np.where(modified_huber(p) >= 0.5, 1, -1)\n",
    "            elif self.loss == \"logistic\":\n",
    "                p = np.sum(pred, axis=0)\n",
    "                pred_final = np.where(logistic(p) >= 0.5, 1, -1)\n",
    "        return pred_final\n",
    "\n",
    "\n",
    "class GBRegression(GradientBoosting):\n",
    "    def __init__(self, M, base_learner, learning_rate, method=\"regression\", loss=\"square\",tol=None, subsample=None, alpha=0.9):\n",
    "        super(GBRegression, self).__init__(M=M, base_learner=base_learner, learning_rate=learning_rate, method=method, \n",
    "                                            loss=loss, tol=tol, subsample=subsample, alpha=alpha)\n",
    "\n",
    "class GBClassification(GradientBoosting):\n",
    "    def __init__(self, M, base_learner, learning_rate, method=\"classification\", loss=\"logistic\", tol=None, subsample=None):\n",
    "        super(GBClassification, self).__init__(M=M, base_learner=base_learner, learning_rate=learning_rate, method=method,\n",
    "                                                loss=loss, tol=tol, subsample=subsample)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # creat the dataset and start training \n",
    "    X, y = datasets.make_regression(n_samples=20000, n_features=10, n_informative=4, noise=1.1, random_state=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    model = GBRegression(M=1000, base_learner=DecisionTreeRegressor(max_depth=2, random_state=1), learning_rate=0.1,\n",
    "                         loss=\"huber\")\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "    print('RMSE: ', rmse)\n",
    "\n",
    "    X, y = datasets.make_classification(n_samples=20000, n_features=10, n_informative=4, flip_y=0.1, \n",
    "                                    n_clusters_per_class=1, n_classes=2, random_state=1)\n",
    "    y[y==0] = -1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    model = GBClassification(M=1000, base_learner=DecisionTreeRegressor(max_depth=1, random_state=1), learning_rate=1.0,\n",
    "                             method=\"classification\", loss=\"logistic\")\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    acc = np.zeros(pred.shape)\n",
    "    acc[np.where(pred == y_test)] = 1\n",
    "    accuracy = np.sum(acc) / len(pred)\n",
    "    print('accuracy logistic score: ', accuracy)\n",
    "\n",
    "    model = GBClassification(M=1000, base_learner=DecisionTreeRegressor(max_depth=1, random_state=1), learning_rate=1.0,\n",
    "                             method=\"classification\", loss=\"modified_huber\")\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    acc = np.zeros(pred.shape)\n",
    "    acc[np.where(pred == y_test)] = 1\n",
    "    accuracy = np.sum(acc) / len(pred)\n",
    "    print('accuracy modified_huber score: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 15)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# read in data\n",
    "heart = pd.read_csv(\"Heart.csv\")\n",
    "heart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPain</th>\n",
       "      <th>RestBP</th>\n",
       "      <th>Chol</th>\n",
       "      <th>Fbs</th>\n",
       "      <th>RestECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExAng</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Ca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>299</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>300</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>301</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>302</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>303</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>138</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Age  Sex     ChestPain  RestBP  Chol  Fbs  RestECG  MaxHR  \\\n",
       "0             1   63    1       typical     145   233    1        2    150   \n",
       "1             2   67    1  asymptomatic     160   286    0        2    108   \n",
       "2             3   67    1  asymptomatic     120   229    0        2    129   \n",
       "3             4   37    1    nonanginal     130   250    0        0    187   \n",
       "4             5   41    0    nontypical     130   204    0        2    172   \n",
       "..          ...  ...  ...           ...     ...   ...  ...      ...    ...   \n",
       "298         299   45    1       typical     110   264    0        0    132   \n",
       "299         300   68    1  asymptomatic     144   193    1        0    141   \n",
       "300         301   57    1  asymptomatic     130   131    0        0    115   \n",
       "301         302   57    0    nontypical     130   236    0        2    174   \n",
       "302         303   38    1    nonanginal     138   175    0        0    173   \n",
       "\n",
       "     ExAng  Oldpeak  Slope   Ca  \n",
       "0        0      2.3      3  0.0  \n",
       "1        1      1.5      2  3.0  \n",
       "2        1      2.6      2  2.0  \n",
       "3        0      3.5      3  0.0  \n",
       "4        0      1.4      1  0.0  \n",
       "..     ...      ...    ...  ...  \n",
       "298      0      1.2      2  0.0  \n",
       "299      0      3.4      2  2.0  \n",
       "300      1      1.2      2  1.0  \n",
       "301      0      0.0      2  1.0  \n",
       "302      0      0.0      1  NaN  \n",
       "\n",
       "[303 rows x 13 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = heart[heart.columns[:-2]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thal</th>\n",
       "      <th>AHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Thal  AHD\n",
       "0         fixed   No\n",
       "1        normal  Yes\n",
       "2    reversable  Yes\n",
       "3        normal   No\n",
       "4        normal   No\n",
       "..          ...  ...\n",
       "298  reversable  Yes\n",
       "299  reversable  Yes\n",
       "300  reversable  Yes\n",
       "301      normal  Yes\n",
       "302      normal   No\n",
       "\n",
       "[303 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = heart[heart.columns[-2:]]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "\n",
    "bst = xgb.train(param, X_train, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import  make_classification\n",
    " \n",
    "print('Load data...')\n",
    "\n",
    "iris = load_iris()\n",
    "data=iris.data\n",
    "target = iris.target\n",
    "X_train,X_test,y_train,y_test =train_test_split(data,target,test_size=0.2)\n",
    "\n",
    "# df_train = pd.read_csv('../regression/regression.train', header=None, sep='\\t')\n",
    "# df_test = pd.read_csv('../regression/regression.test', header=None, sep='\\t')\n",
    "# y_train = df_train[0].values\n",
    "# y_test = df_test[0].values\n",
    "# X_train = df_train.drop(0, axis=1).values\n",
    "# X_test = df_test.drop(0, axis=1).values\n",
    "\n",
    "print('Start training...')\n",
    "# Creat model\n",
    "gbm = lgb.LGBMRegressor(objective='regression',num_leaves=31,learning_rate=0.05,n_estimators=20)\n",
    "gbm.fit(X_train, y_train,eval_set=[(X_test, y_test)],eval_metric='l1',early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "# give prediction\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "\n",
    "# test the model\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importances_))\n",
    "\n",
    "# opitmization\n",
    "estimator = lgb.LGBMRegressor(num_leaves=31)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(estimator, param_grid)\n",
    "\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters found by grid search are:', gbm.best_params_) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
